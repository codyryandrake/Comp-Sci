To create a user-oriented, publically available tool for generating uniquely infinite animation assets for the user to retain for reuse or manipulation. Generated assets may also be applied to sculptural light art animation, pre-rendered or in real-time. 


Tools and References:
-Processing Family (P5, Processing)
-HTML/CSS
-Daniel Shirfmann (sp?) 
-The Nature of Code
TEXTURE GENERATOR
-https://github.com/mxgmn/WaveFunctionCollapse
COMPUTER VISION/LED MAPPER
-https://github.com/dfashbaugh/NeoPixelControlTools/blob/master
/LEDVideoMapper/testWebcamOpenCV.py
	OPEN CV
	-https://github.com/abidrahmank/OpenCV2-Python-Tutorials/blob/master/source/py_tutorials/py_tutorials.rst
-Fadecandy Pixel Driver
-Client Workstation (RPi or Dedicated PC)
-Sensor Matrix System 
-Github 
-Adafruit
-Fadecandy neopixel controller w/dithering
-Scanlime (Fadecandy Github Docs and Implementation)


TOP LAYER:
-Web App w/Input Options 
	-Input sample pixels into generative model
	-Draw/Click on the screen to generate desired tempo
	-Create array of generative animations and let user pick
	|| 
-Sculpture Sensor Input w/Feedback
	-Detect user presence with PIR or IR sensors and Capacitive Touch
	-Detect environmental sound quality
-Allow for user placement of force systems on canvas or sculpture (attraction, repulsion, etc.)

	MIDDLE LAYER (HARDWARE)
	-Micro controller receives sensor readings from User and passes them to Processing application.
	-Fadecandy receives output from Processing application and pushes data to WS2812b LEDs.  

	MIDDLE LAYER (SOFTWARE)
	-Processing Canvas Rendered and Pushed to HTML Page
		|| Processing Canvas Rendered and Pushed to FadeCandy. 
	-